{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 0. CHECK GPU (Must be run first)\n",
                "import torch\n",
                "try:\n",
                "    print(\"ðŸ” Checking GPU Status...\")\n",
                "    !nvidia-smi\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"âœ… GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
                "    else:\n",
                "        raise RuntimeError(\"âŒ No GPU detected! Please go to Runtime > Change runtime type > T4 GPU\")\n",
                "except Exception as e:\n",
                "    print(str(e))\n",
                "    # Stop execution if no GPU\n",
                "    raise e\n",
                "\n",
                "# 1. Install Dependencies\n",
                "!pip install -q --upgrade torch torchvision torchaudio\n",
                "!pip install -q --upgrade transformers bitsandbytes accelerate\n",
                "!pip install -q --upgrade \"fastapi>=0.115.0,<0.124.0\" uvicorn pyngrok nest-asyncio\n",
                "\n",
                "# 2. Run Server code\n",
                "import os\n",
                "import traceback\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from fastapi import FastAPI, Request, HTTPException\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "import uvicorn\n",
                "import nest_asyncio\n",
                "from pyngrok import ngrok\n",
                "import threading\n",
                "import asyncio\n",
                "\n",
                "# --- CONFIG ---\n",
                "MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\" \n",
                "NGROK_TOKEN = \"à¹ƒà¸ªà¹ˆ_NGROK_TOKEN_à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸•à¸£à¸‡à¸™à¸µà¹‰\"\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "# Enable CORS\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=[\"*\"],\n",
                "    allow_credentials=True,\n",
                "    allow_methods=[\"*\"],\n",
                "    allow_headers=[\"*\"],\n",
                ")\n",
                "\n",
                "print(\"â³ Initializing Model (Phase 1/3)...\")\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "try:\n",
                "    if not torch.cuda.is_available():\n",
                "        raise RuntimeError(\"No GPU available for model loading!\")\n",
                "\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config, \n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    print(\"âœ… Model Loaded Successfully! (Phase 2/3)\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Model Load Failed: {e}\")\n",
                "    traceback.print_exc()\n",
                "\n",
                "@app.post(\"/chat\")\n",
                "async def chat(request: Request):\n",
                "    try:\n",
                "        data = await request.json()\n",
                "        message = data.get(\"message\", \"\")\n",
                "        persona = data.get(\"persona\", \"\") \n",
                "        \n",
                "        print(f\"ðŸ“© Received: {message[:50]}...\")\n",
                "        \n",
                "        # --- PROMPT CONSTRUCTION (LLAMA 3 FORMAT) ---\n",
                "        # Use standard Llama 3 Instruct tags\n",
                "        input_text = (\n",
                "            f\"<|begin_of_text|>\"\n",
                "            f\"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
                "            f\"{persona}\\n\"\n",
                "            f\"<|eot_id|>\"\n",
                "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
                "            f\"{message}\\n\"\n",
                "            f\"<|eot_id|>\"\n",
                "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
                "        )\n",
                "        \n",
                "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                **inputs, \n",
                "                max_new_tokens=256,\n",
                "                tokenizer=tokenizer,\n",
                "                stop_strings=[\"<|eot_id|>\", \"<|end_of_text|>\"] # Native stops\n",
                "            )\n",
                "        \n",
                "        reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        # Cleanup Llama 3 artifacts\n",
                "        # Standard decoding might leave the full prompt. We strip it.\n",
                "        if \"assistant\" in reply:\n",
                "            reply = reply.split(\"assistant\")[-1].strip()\n",
                "        \n",
                "        # Manual clean cut for any leaked headers\n",
                "        for stop_word in [\"User:\", \"Mali:\", \"<|eot_id|>\", \"<|start_header_id|>\"]:\n",
                "            if stop_word in reply:\n",
                "                reply = reply.split(stop_word)[0].strip()\n",
                "\n",
                "        return {\"reply\": reply}\n",
                "\n",
                "    except Exception as e:\n",
                "        print(\"ðŸ”¥ generation error (Detail below):\")\n",
                "        traceback.print_exc()\n",
                "        return {\"reply\": f\"Server Core Error: {str(e)}\"}\n",
                "\n",
                "# Start Server\n",
                "print(\"ðŸš€ Starting Server (Phase 3/3)...\")\n",
                "ngrok.set_auth_token(NGROK_TOKEN)\n",
                "public_url = ngrok.connect(8000).public_url\n",
                "print(f\"ðŸ”— Public URL: {public_url}\")\n",
                "print(\"à¹ƒà¸«à¹‰à¸™à¸³ URL à¸™à¸µà¹‰à¹„à¸›à¹ƒà¸ªà¹ˆà¹ƒà¸™à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² AInote à¹„à¸”à¹‰à¹€à¸¥à¸¢à¸„à¸£à¸±à¸š!\")\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "config = uvicorn.Config(app, port=8000)\n",
                "server = uvicorn.Server(config)\n",
                "await server.serve()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}